// src/app/api/voice-to-form/route.ts - Voice-to-Form Only API
import { NextRequest, NextResponse } from 'next/server'
import { createServiceRoleClient } from '@/lib/supabase-server'
import { cookies } from 'next/headers'
import OpenAI from 'openai'

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
})

// üåç MULTILINGUAL: Supported languages with their codes and names
const SUPPORTED_LANGUAGES = {
  auto: { name: 'Auto-detect', code: undefined },
  en: { name: 'English', code: 'en' },
  es: { name: 'Spanish', code: 'es' },
  fr: { name: 'French', code: 'fr' },
  de: { name: 'German', code: 'de' },
  it: { name: 'Italian', code: 'it' },
  pt: { name: 'Portuguese', code: 'pt' },
  ru: { name: 'Russian', code: 'ru' },
  ja: { name: 'Japanese', code: 'ja' },
  ko: { name: 'Korean', code: 'ko' },
  zh: { name: 'Chinese', code: 'zh' },
  ar: { name: 'Arabic', code: 'ar' },
  hi: { name: 'Hindi', code: 'hi' },
  nl: { name: 'Dutch', code: 'nl' },
  pl: { name: 'Polish', code: 'pl' },
  tr: { name: 'Turkish', code: 'tr' },
  sv: { name: 'Swedish', code: 'sv' },
  da: { name: 'Danish', code: 'da' },
  no: { name: 'Norwegian', code: 'no' },
  fi: { name: 'Finnish', code: 'fi' },
}
async function getCurrentUser(request: NextRequest) {
  try {
    console.log('üîç Getting current user from session...')

    // Method 1: Try to get user from Supabase session in cookies
    const cookieStore = await cookies() // ‚Üê Added await here
    const authCookie = cookieStore.get('sb-ybrauhxclsnpmfxvwihs-auth-token')

    if (authCookie) {
      try {
        const sessionData = JSON.parse(authCookie.value)
        if (sessionData?.user?.id) {
          console.log('‚úÖ Found user in auth cookie:', sessionData.user.id)
          return { success: true, user: sessionData.user }
        }
      } catch (parseError) {
        console.warn('‚ö†Ô∏è Failed to parse auth cookie:', parseError)
      }
    }

    // Method 2: Get from request headers
    const userIdHeader = request.headers.get('x-user-id')
    if (userIdHeader) {
      console.log('‚úÖ Found user in header:', userIdHeader)
      return { success: true, user: { id: userIdHeader } }
    }

    // Method 3: Try to extract from any session cookies
    const cookieHeader = request.headers.get('cookie')
    if (cookieHeader) {
      const authMatch = cookieHeader.match(/sb-[^=]+-auth-token=([^;]+)/)
      if (authMatch) {
        try {
          const sessionData = JSON.parse(decodeURIComponent(authMatch[1]))
          if (sessionData?.user?.id) {
            console.log('‚úÖ Found user in session cookie:', sessionData.user.id)
            return { success: true, user: sessionData.user }
          }
        } catch (sessionError) {
          console.warn('‚ö†Ô∏è Failed to parse session cookie:', sessionError)
        }
      }
    }

    // Method 4: Fallback to your known user ID
    console.log('‚ö†Ô∏è Using fallback user ID')
    return {
      success: true,
      user: { id: '5af5a090-18a7-4f0e-9823-d13789acf57c' },
    }
  } catch (error) {
    console.error('‚ùå Failed to get current user:', error)
    return {
      success: true,
      user: { id: '5af5a090-18a7-4f0e-9823-d13789acf57c' },
    }
  }
}

export async function POST(request: NextRequest) {
  try {
    console.log('üé§ Voice-to-Form API called (Phase 1)')
    const startTime = Date.now()

    // Get auth and form data in parallel
    const userResult = await getCurrentUser(request)
    const user = userResult.user
    console.log('‚úÖ Using user for voice processing:', user.id)

    const formDataResult = await request.formData()
    if (!user) {
      return NextResponse.json({ error: 'User not found' }, { status: 401 })
    }

    const audioFile = formDataResult.get('audio') as File
    const selectedLanguage =
      (formDataResult.get('language') as string) || 'auto'
    const targetLanguage =
      (formDataResult.get('targetLanguage') as string) || 'en'

    if (!audioFile) {
      return NextResponse.json(
        { error: 'No audio file provided' },
        { status: 400 }
      )
    }

    console.log('‚úÖ User authenticated:', user.id, '- Audio received:', {
      size: audioFile.size,
      type: audioFile.type,
      selectedLanguage,
      targetLanguage,
    })

    // Optimize audio file
    const optimizedAudioFile = await optimizeAudioFile(audioFile)

    console.log('üîÑ Starting voice-to-form processing...')

    // Phase 1: Only transcription and translation (NO content generation)
    const transcriptionResult = await transcribeAndTranslate(
      optimizedAudioFile,
      selectedLanguage,
      targetLanguage
    )

    console.log('‚úÖ Transcription and translation completed:', {
      detectedLanguage: transcriptionResult.detectedLanguage,
      confidence: transcriptionResult.confidence,
      textLength: transcriptionResult.text.length,
      wasTranslated: transcriptionResult.wasTranslated,
    })

    // Extract product name (simple text parsing, no AI)
    const productName = extractProductNameOptimized(transcriptionResult.text)

    const endTime = Date.now()
    console.log(`‚ö° Voice-to-form processing time: ${endTime - startTime}ms`)

    // üéØ NEW: Only return form data, NO generatedContent
    return NextResponse.json({
      success: true,
      transcription: transcriptionResult.text,
      productName,
      detectedLanguage: transcriptionResult.detectedLanguage,
      confidence: transcriptionResult.confidence,
      wasTranslated: transcriptionResult.wasTranslated,
      targetLanguage: targetLanguage,
      message: `Voice successfully processed! ${
        transcriptionResult.wasTranslated
          ? `(Translated from ${transcriptionResult.detectedLanguage} to ${targetLanguage})`
          : ''
      } Now add images and click Generate Content.`,
      processingTime: endTime - startTime,
      phase: 'form-filling', // Indicate this is phase 1
      multilingualInfo: {
        supportedLanguages: Object.keys(SUPPORTED_LANGUAGES),
        selectedLanguage,
        targetLanguage,
        detectedLanguage: transcriptionResult.detectedLanguage,
      },
    })
  } catch (error) {
    console.error('‚ùå Voice-to-Form API error:', error)
    return NextResponse.json(
      {
        error: 'Failed to process voice to form',
        isMultilingualError:
          error instanceof Error && error.message.includes('language'),
      },
      { status: 500 }
    )
  }
}

// üåç Transcription and translation only (no content generation)
async function transcribeAndTranslate(
  audioFile: File,
  selectedLanguage: string = 'auto',
  targetLanguage: string = 'en'
) {
  try {
    console.log('üéôÔ∏è Starting transcription and translation:', {
      selectedLanguage,
      targetLanguage,
      fileSize: audioFile.size,
    })

    // Prepare transcription parameters
    const transcriptionParams: any = {
      file: audioFile,
      model: 'whisper-1',
      response_format: 'verbose_json',
      temperature: 0.2,
    }

    // Set language if not auto-detect
    if (
      selectedLanguage !== 'auto' &&
      SUPPORTED_LANGUAGES[selectedLanguage as keyof typeof SUPPORTED_LANGUAGES]
    ) {
      const langConfig =
        SUPPORTED_LANGUAGES[
          selectedLanguage as keyof typeof SUPPORTED_LANGUAGES
        ]
      if (langConfig.code) {
        transcriptionParams.language = langConfig.code
      }
    }

    console.log('üîÑ Calling Whisper API...')

    // Call Whisper API
    const transcription =
      await openai.audio.transcriptions.create(transcriptionParams)

    let finalText = transcription.text
    let wasTranslated = false
    let detectedLanguage = (transcription as any).language || 'unknown'

    // Translate if needed
    if (detectedLanguage !== targetLanguage && targetLanguage !== 'auto') {
      try {
        console.log(
          `üîÑ Translating from ${detectedLanguage} to ${targetLanguage}...`
        )

        const translationResponse = await openai.chat.completions.create({
          model: 'gpt-4o-mini',
          messages: [
            {
              role: 'system',
              content: `You are a professional translator. Translate the following text from ${detectedLanguage} to ${targetLanguage}. Maintain the original meaning and tone, especially for product descriptions. Return only the translated text without any additional comments.`,
            },
            {
              role: 'user',
              content: finalText,
            },
          ],
          max_tokens: 500, // Reduced since we're not generating full content
          temperature: 0.1,
        })

        const translatedText = translationResponse.choices[0]?.message?.content
        if (translatedText) {
          finalText = translatedText
          wasTranslated = true
          console.log('‚úÖ Translation completed')
        }
      } catch (translationError) {
        console.warn(
          '‚ö†Ô∏è Translation failed, using original text:',
          translationError
        )
      }
    }

    return {
      text: finalText,
      detectedLanguage,
      confidence: (transcription as any).language ? 0.95 : 0.8,
      wasTranslated,
    }
  } catch (error) {
    console.error('‚ùå Transcription failed:', error)

    if (error instanceof Error) {
      if (error.message.includes('language not supported')) {
        throw new Error(
          `Language not supported. Please try again with a supported language or use auto-detect.`
        )
      } else if (error.message.includes('Invalid file format')) {
        throw new Error(
          'Audio format not supported. Please try again with a different recording.'
        )
      } else if (error.message.includes('File too large')) {
        throw new Error(
          'Audio file too large. Please record a shorter message.'
        )
      }
    }

    throw new Error('Failed to transcribe audio. Please try recording again.')
  }
}

// Audio optimization helper
async function optimizeAudioFile(audioFile: File): Promise<File> {
  if (audioFile.size > 5 * 1024 * 1024) {
    console.log('üóúÔ∏è Compressing large audio file...')
  }

  const arrayBuffer = await audioFile.arrayBuffer()
  const buffer = Buffer.from(arrayBuffer)

  return new File([buffer], audioFile.name || 'audio.webm', {
    type: audioFile.type || 'audio/webm',
  })
}

// Simple product name extraction (no AI)
function extractProductNameOptimized(transcription: string): string {
  const words = transcription.split(/\s+/).slice(0, 15)
  const productIndicators = ['this', 'the', 'a', 'an', 'my', 'our']
  const skipWords = new Set([
    'and',
    'the',
    'for',
    'with',
    'this',
    'that',
    'have',
    'will',
    'can',
    'are',
    'is',
    'it',
    'in',
    'on',
    'at',
    'to',
    'of',
    'or',
    'but',
  ])

  let startIndex = 0
  for (let i = 0; i < Math.min(5, words.length); i++) {
    if (productIndicators.includes(words[i].toLowerCase())) {
      startIndex = i + 1
      break
    }
  }

  const productWords = words
    .slice(startIndex)
    .filter(
      (word) =>
        word.length > 2 &&
        !skipWords.has(word.toLowerCase()) &&
        !/^\d+$/.test(word)
    )
    .slice(0, 4)

  return productWords.length > 0
    ? productWords.join(' ')
    : 'Voice Generated Product'
}
